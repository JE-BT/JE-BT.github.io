---
layout: post
title: Risk and Generative AI
excerpt: "Doomsday AI scenarios reflect more about our fears and institutions than actual machine autonomy. What if the real danger isn’t runaway intelligence—but how we react to it?"
modified: 8/8/2025, 09:00
tags: [ai, ethics, governance, risk]
comments: true
category: blog
---

# From Golems to Goodwill: Why AI Risk Is More About Us Than the Machines

If you've been watching this week’s announcements in the AI world—new open models, deals with governments, and growing regulatory heat—you might be wondering: **is this the beginning of the end, or just another chapter in how humans adapt to new tools?**

The answer might depend on whether you think AI systems are becoming *autonomous agents*—or whether they’re just *very convincing interfaces* to the tools we already built.

## The “Doomsday” Bias

Much of the modern AI risk discourse leans into adversarial metaphors: rogue agents, runaway superintelligence, existential takeoff. These scenarios—popularized in documents like *AI 2027*—often assume machines will start making their own goals and outsmart humans.  

But there's a flaw in that logic. Today’s most capable AIs (even the ones you’re chatting with) **don’t think or plan independently**. They're more like interactive glue between language and automation—what Bruno Latour might call “nature–culture hybrids.” Their “agency” is borrowed from how humans use them.

We aren’t staring at Skynet. We’re staring at ourselves.

## It’s Not the Machine—It’s the System

Empirical data from AI deployments—open and closed—suggests the biggest risks come from human decisions:
- **Over-reliance on LLMs** in healthcare, finance, or military decisions without understanding limitations.
- **Misinformation and bias** amplified at scale.
- **Governments and companies** racing to deploy AI for strategic advantage without safeguards.

In other words, AI’s impact is sociotechnical: it emerges from a **network of humans, machines, institutions, and incentives**.

This fits what sociologist Émile Durkheim described as *social facts*—beliefs that shape reality because people act as if they’re true. If people believe AI is infallible or autonomous, they may hand it power it doesn’t deserve.

## Lewis, Latour, and Goodall: A Different Story

C.S. Lewis warned about the “Abolition of Man”—the danger of power without morality. Jane Goodall taught us that intelligent systems (even organic ones) are shaped by social context—capable of both cruelty and compassion.

And Latour would remind us: AI isn’t “out there” as an alien force. It’s here, in our labs, contracts, regulatory memos, and public imaginations.

The real risk isn’t that AI *becomes like us*—it’s that we forget to **be ourselves** in how we design, deploy, and regulate it.

## So What Should We Do?

If the risk is sociotechnical—not just technical—then our response should be too:
- **Create accountability structures** that trace harm back to decisions, not just algorithms.
- **Train people in moral reasoning**, not just prompt engineering.
- **Foster international norms** that balance safety, openness, and strategic stability.
- **Recognize cultural and moral pluralism** in AI use—there’s no single “safe path,” but many.

AI may be fast, but *culture moves first*.

## Final Thought: Maybe We’ve Been Asking the Wrong Question

Alan Turing asked whether machines could think. Today we should ask:  
**Can we think clearly about machines—without projecting our fears onto them?**

The future of AI may not hinge on a technological singularity, but on something more ancient: our capacity for responsibility, cooperation, and imagination.

# Summary

Contemporary discourse on artificial intelligence (AI) risk, particularly in Western contexts, is dominated by scenarios in which machine autonomy accelerates beyond human control, leading to catastrophic outcomes. Doomsday prophecies like "AI 2027" rest on the assumption that large language models (LLMs) exhibit emergent, self-directed agency. Looking at these risks sociotechnically provides a new perspective: these fears arise from the interplay of human actors, institutional structures, and technical systems, rather than from autonomous machine will. This blog post critiques the adversarial framing embedded in popular AI "doomsday" narratives and returns to the moral imaginings of C.S. Lewis, the sociology of Durkheim, and the nature-culture hybrids written of by Latour. These scholars provide various perspectives on human and machine action such as those written of by Jane Goodall and Alan Turing. The ultimate argument here is that AI's trajectory will be shaped less by runaway autonomy than by moral human agency, institutional incentives, and cultural narratives. This human-centered framing neither dismisses risk nor indulges in technological fatalism, but repositions these fears within a risk framework built on enduring dynamics of cooperation, conflict, and moral negotiation.

This blog post was written with the help of Open AI's new model: GPT-5.

---

## Sources and Further Reading

- Latour, B. (1993). *We Have Never Been Modern*. Harvard University Press.  
- Lewis, C.S. (1943). *The Abolition of Man*. Oxford University Press.  
- Durkheim, É. (1895/1982). *The Rules of Sociological Method*. Free Press.  
- Goodall, J. (1986). *The Chimpanzees of Gombe*. Belknap Press.  
- Turing, A.M. (1950). *Computing Machinery and Intelligence*. *Mind*, 59(236), 433–460.  
- Whittlestone, J., & Avin, S. (2024). *Governing General Purpose AI*. Cambridge University.

---
*Last updated: August 08, 2025*

**Contact Information**   
Email: jason.e.bunderson@nord.no
Blue-Sky: [@jebtoler](https://bsky.app/profile/jebtoler.bsky.social)
