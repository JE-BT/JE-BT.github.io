---
layout: post
title: The Risk of Generative AI
excerpt: "Doomsday AI scenarios reflect more about our fears and institutions than actual machine autonomy. What if the real danger isn’t runaway intelligence—but how we react to it?"
modified: 8/8/2025, 09:00
tags: [ai, ethics, governance, risk]
comments: true
category: blog
---

# The Risk of Generative AI

This week, Open AI released their newest Generative AI model to the world: GPT 5 (read the [accouncement](https://openai.com/index/introducing-gpt-5/)). Their annoucement comes at a time of increased anxiety over the role of AI in our society. Open AI also announced this week a deal to sell their model to the US government and release a model with open weights and biases that can be run on a personal computer by hobbyists and innovators (read more about their sale to the [US Government](https://openai.com/index/providing-chatgpt-to-the-entire-us-federal-workforce/) or [Open Models](https://openai.com/open-models/)). Many of the most headline-catching concerns, such as the AI 2027 scenario released by several key AI researchers (read it [here](https://ai-2027.com/)), evaluate the possibility of AI going rogue or 'mis-aligned,' taking human instructions and twisting them in some way to create a negative outcome. But just as worrying are reports of misuse by business and governments who use AI to, among other things, persecute minorities (in both [China](https://thebulletin.org/2022/10/chinas-high-tech-surveillance-drives-oppression-of-uyghurs/) and the [United States](https://www.law.georgetown.edu/immigration-law-journal/wp-content/uploads/sites/19/2020/08/Weapons-of-Mass-Deportation-Big-Data-and-Automated-Decision-Making-Systems-in-Immigration-Law.pdf)) and create and sell AI-directed weapons systems ([Palantir](https://www.palantir.com/offerings/defense/)).

Reading these reports, it is easy to question if this is actually the beginning of some catastrophic reordering of society rather than just another new innovation to add to the toolbox. However, if we move away from the idea of AI systems as *autonomous agents* and towards the idea of Generative AI as a *very convincing interface* which uses tools already built, the risk landscape looks different.

The remainder of this post is in a 'blogger's voice' generated by the new GPT-5. I gave it some relevant readings on sociotechnical risk and, after a contextualizing discussion, this is how it rewrote these ideas:

## The “Doomsday” Bias

Much of the modern AI risk discourse leans into adversarial metaphors: rogue agents, runaway superintelligence, existential takeoff. These scenarios—popularized in documents like *AI 2027*—often assume machines will start making their own goals and outsmart humans.  

But there's a flaw in that logic. Today’s most capable AIs (even the ones you’re chatting with) **don’t think or plan independently**. They're more like interactive glue between language and automation—what Bruno Latour might call “nature–culture hybrids.” Their “agency” is borrowed from how humans use them.

We aren’t staring at Skynet. We’re staring at ourselves.

## It’s Not the Machine—It’s the System

Empirical data from AI deployments—open and closed—suggests the biggest risks come from human decisions:
- **Over-reliance on LLMs** in healthcare, finance, or military decisions without understanding limitations.
- **Misinformation and bias** amplified at scale.
- **Governments and companies** racing to deploy AI for strategic advantage without safeguards.

In other words, AI’s impact is sociotechnical: it emerges from a **network of humans, machines, institutions, and incentives**.

This fits what sociologist Émile Durkheim described as *social facts*—beliefs that shape reality because people act as if they’re true. If people believe AI is infallible or autonomous, they may hand it power it doesn’t deserve.

## Lewis, Latour, and Goodall: A Different Story

C.S. Lewis warned about the “Abolition of Man”—the danger of power without morality. Jane Goodall taught us that intelligent systems (even organic ones) are shaped by social context—capable of both cruelty and compassion.

And Latour would remind us: AI isn’t “out there” as an alien force. It’s here, in our labs, contracts, regulatory memos, and public imaginations.

The real risk isn’t that AI *becomes like us*—it’s that we forget to **be ourselves** in how we design, deploy, and regulate it.

## So What Should We Do?

If the risk is sociotechnical—not just technical—then our response should be too:
- **Create accountability structures** that trace harm back to decisions, not just algorithms.
- **Train people in moral reasoning**, not just prompt engineering.
- **Foster international norms** that balance safety, openness, and strategic stability.
- **Recognize cultural and moral pluralism** in AI use—there’s no single “safe path,” but many.

AI may be fast, but *culture moves first*.

## Final Thought: Maybe We’ve Been Asking the Wrong Question

Alan Turing asked whether machines could think. Today we should ask:  
**Can we think clearly about machines—without projecting our fears onto them?**

The future of AI may not hinge on a technological singularity, but on something more ancient: our capacity for responsibility, cooperation, and imagination.

# Summary

Ending with a human-AI hybrid text, here is what I get out of this new development.

Contemporary discourse on artificial intelligence (AI) risk, particularly in Western contexts, is dominated by scenarios in which machine autonomy accelerates beyond human control, leading to catastrophic outcomes. Doomsday prophecies like "AI 2027" rest on the assumption that large language models (LLMs) exhibit emergent, self-directed agency. Looking at these risks sociotechnically provides a new perspective: these fears arise from the interplay of human actors, institutional structures, and technical systems, rather than from autonomous machine will. This blog post critiques the adversarial framing embedded in popular AI "doomsday" narratives and returns to the moral imaginings of C.S. Lewis, the sociology of Durkheim, and the nature-culture hybrids written of by Latour. These scholars provide various perspectives on human and machine action such as those written of by Jane Goodall and Alan Turing. The ultimate argument here is that AI's trajectory will be shaped less by runaway autonomy than by moral human agency, institutional incentives, and cultural narratives. This human-centered framing neither dismisses risk nor indulges in technological fatalism, but repositions these fears within a risk framework built on enduring dynamics of cooperation, conflict, and moral negotiation.

This blog post was written with the help of Open AI's new model: GPT-5.

---

## Sources and Further Reading

- Latour, B. (1993). *We Have Never Been Modern*. Harvard University Press.  
- Lewis, C.S. (1943). *The Abolition of Man*. Oxford University Press.  
- Durkheim, É. (1895/1982). *The Rules of Sociological Method*. Free Press.  
- Goodall, J. (1986). *The Chimpanzees of Gombe*. Belknap Press.  
- Turing, A.M. (1950). *Computing Machinery and Intelligence*. *Mind*, 59(236), 433–460.  
- Whittlestone, J., & Avin, S. (2024). *Governing General Purpose AI*. Cambridge University.
- [OpenAI GPT-5 Announcement](https://openai.com/index/introducing-gpt-5/)  
- [OpenAI Deal with US Government](https://openai.com/index/providing-chatgpt-to-the-entire-us-federal-workforce/)  
- [OpenAI Open Models Release](https://openai.com/open-models/)  
- [AI 2027 Scenario](https://ai-2027.com/)  
- [China’s High-Tech Surveillance of Uyghurs](https://thebulletin.org/2022/10/chinas-high-tech-surveillance-drives-oppression-of-uyghurs/)  
- [Automated Decision-Making in US Immigration](https://www.law.georgetown.edu/immigration-law-journal/wp-content/uploads/sites/19/2020/08/Weapons-of-Mass-Deportation-Big-Data-and-Automated-Decision-Making-Systems-in-Immigration-Law.pdf)  
- [Palantir Defense Offerings](https://www.palantir.com/offerings/defense/)

---
*Last updated: August 08, 2025*

**Contact Information**   
Email: jason.e.bunderson@nord.no
Blue-Sky: [@jebtoler](https://bsky.app/profile/jebtoler.bsky.social)
